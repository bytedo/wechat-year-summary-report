"""
vector_engine.py - å‘é‡åˆ†æå¼•æ“æ¨¡å—

åŸºäº Sentence-Transformers å®ç°è¯­ä¹‰åˆ†æï¼š
1. æ–‡æœ¬å‘é‡åŒ– (Embedding)
2. K-Means èšç±»
3. t-SNE é™ç»´å¯è§†åŒ–
"""

import hashlib
import os
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from tqdm import tqdm


class SemanticAnalyzer:
    """
    è¯­ä¹‰åˆ†æå™¨ï¼šå®ç°æ–‡æœ¬å‘é‡åŒ–ã€èšç±»å’Œé™ç»´ã€‚
    """
    
    # é»˜è®¤æ¨¡å‹åç§°ï¼ˆè½»é‡çº§ä¸­æ–‡æ¨¡å‹ï¼‰
    DEFAULT_MODEL = "BAAI/bge-small-zh-v1.5"
    
    def __init__(
        self,
        model_name: str = None,
        n_clusters: int = 6,
        cache_dir: str = None,
        min_content_length: int = 5,
        use_gpu: bool = True
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†æå™¨ã€‚
        
        å‚æ•°:
            model_name: SentenceTransformer æ¨¡å‹åç§°
            n_clusters: èšç±»æ•°é‡ï¼ˆé»˜è®¤ 6ï¼‰
            cache_dir: å‘é‡ç¼“å­˜ç›®å½•
            min_content_length: æœ€å°æ–‡æœ¬é•¿åº¦ï¼ˆè¿‡çŸ­çš„æ¶ˆæ¯ä¸å‚ä¸åˆ†æï¼‰
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆè‡ªåŠ¨æ£€æµ‹å¯ç”¨æ€§ï¼‰
        """
        self.model_name = model_name or self.DEFAULT_MODEL
        self.n_clusters = n_clusters
        self.min_content_length = min_content_length
        self.use_gpu = use_gpu
        
        # æ£€æµ‹ GPU å¯ç”¨æ€§
        self.device = self._detect_device()
        
        # ç¼“å­˜ç›®å½•
        # ç¼“å­˜ç›®å½•
        if cache_dir is None:
            # é»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ .cache/vectors
            cache_dir = Path(__file__).parent.parent / ".cache" / "vectors"
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹ç¼“å­˜ç›®å½•
        self.model_cache_dir = self.cache_dir.parent / "models"
        self.model_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹å»¶è¿ŸåŠ è½½
        self._model = None
    
    def _detect_device(self) -> str:
        """æ£€æµ‹å¯ç”¨è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰"""
        if not self.use_gpu:
            return "cpu"
        
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                print(f"   ğŸ® æ£€æµ‹åˆ° GPU: {gpu_name}")
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                print("   ğŸ æ£€æµ‹åˆ° Apple Silicon GPU")
                return "mps"
            else:
                print("   ğŸ’» æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU")
                return "cpu"
        except ImportError:
            print("   âš ï¸ æœªå®‰è£… PyTorchï¼Œä½¿ç”¨ CPU")
            return "cpu"
    
    @property
    def model(self):
        """å»¶è¿ŸåŠ è½½ SentenceTransformer æ¨¡å‹ã€‚"""
        if self._model is None:
            print(f"   ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡æ¨¡å‹ ({self.model_name})...")
            print("   â³ é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 67MBï¼‰...")
            
            try:
                from sentence_transformers import SentenceTransformer
                self._model = SentenceTransformer(
                    self.model_name, 
                    device=self.device,
                    cache_folder=str(self.model_cache_dir)
                )
                print(f"   âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {self.device.upper()})")
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        return self._model
    
    def analyze(
        self,
        df: pd.DataFrame,
        use_cache: bool = True
    ) -> Dict:
        """
        æ‰§è¡Œå®Œæ•´çš„è¯­ä¹‰åˆ†ææµç¨‹ã€‚
        
        å‚æ•°:
            df: æ¶ˆæ¯æ•°æ® DataFrameï¼ˆéœ€åŒ…å« user, content åˆ—ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨å‘é‡ç¼“å­˜
            
        è¿”å›:
            åŒ…å«èšç±»ç»“æœå’Œå¯è§†åŒ–æ•°æ®çš„å­—å…¸
        """
        # è¿‡æ»¤æœ‰æ•ˆæ¶ˆæ¯
        valid_df = self._filter_valid_messages(df)
        
        if len(valid_df) < self.n_clusters * 2:
            print(f"   âš ï¸ æœ‰æ•ˆæ¶ˆæ¯æ•°é‡ä¸è¶³ï¼ˆ{len(valid_df)}æ¡ï¼‰ï¼Œè·³è¿‡è¯­ä¹‰åˆ†æ")
            return self._empty_result()
        
        print(f"   âœ“ æœ‰æ•ˆæ¶ˆæ¯: {len(valid_df)} æ¡")
        
        # Step 1: å‘é‡åŒ–
        contents = valid_df['content'].tolist()
        embeddings = self._get_embeddings(contents, use_cache)
        
        # Step 2: èšç±»
        cluster_labels, cluster_centers = self._cluster(embeddings)
        
        # Step 3: é™ç»´ (ä»…ç”¨äºå¯è§†åŒ–ï¼Œæ•°æ®é‡å¤§æ—¶é‡‡æ ·)
        MAX_VIS_SAMPLES = 8000  # å¯è§†åŒ–æœ€å¤§é‡‡æ ·æ•°
        n_samples = len(embeddings)
        
        if n_samples > MAX_VIS_SAMPLES:
            print(f"   ğŸ“‰ æ•°æ®é‡è¾ƒå¤§ ({n_samples})ï¼Œéšæœºé‡‡æ · {MAX_VIS_SAMPLES} æ¡ç”¨äºå¯è§†åŒ–...")
            np.random.seed(42)
            vis_indices = np.random.choice(n_samples, MAX_VIS_SAMPLES, replace=False)
            vis_embeddings = embeddings[vis_indices]
        else:
            vis_indices = np.arange(n_samples)
            vis_embeddings = embeddings
            
        coords_2d = self._reduce_dimensions(vis_embeddings)
        
        # æ„å»ºç»“æœ
        result = self._build_result(
            valid_df, embeddings, cluster_labels, cluster_centers, 
            coords_2d, vis_indices
        )
        
        return result
    
    def _filter_valid_messages(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤æœ‰æ•ˆæ¶ˆæ¯ï¼ˆé•¿åº¦ > min_content_lengthï¼‰ã€‚"""
        mask = df['content'].str.len() > self.min_content_length
        return df[mask].reset_index(drop=True)
    
    def _get_embeddings(
        self,
        contents: List[str],
        use_cache: bool = True
    ) -> np.ndarray:
        """
        è·å–æ–‡æœ¬å‘é‡ï¼Œæ”¯æŒç¼“å­˜ã€‚
        
        å‚æ•°:
            contents: æ–‡æœ¬åˆ—è¡¨
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        è¿”å›:
            å‘é‡çŸ©é˜µ (n_samples, n_features)
        """
        # è®¡ç®—å†…å®¹å“ˆå¸Œç”¨äºç¼“å­˜
        cache_key = self._compute_cache_key(contents)
        cache_path = self.cache_dir / f"vectors_{cache_key}.pkl"
        
        # å°è¯•åŠ è½½ç¼“å­˜
        if use_cache and cache_path.exists():
            print("   ğŸ“ åŠ è½½å·²ç¼“å­˜çš„å‘é‡æ•°æ®...")
            try:
                with open(cache_path, 'rb') as f:
                    cached = pickle.load(f)
                    cached_embeddings = cached.get('embeddings')
                    if (cached.get('model') == self.model_name and 
                        cached_embeddings is not None and 
                        len(cached_embeddings) == len(contents)):
                        print(f"   âœ“ å·²åŠ è½½ {len(contents)} æ¡æ¶ˆæ¯çš„å‘é‡ç¼“å­˜")
                        return cached_embeddings
                    else:
                        print(f"   âš ï¸ ç¼“å­˜å¤±æ•ˆ (æ¨¡å‹æˆ–æ•°é‡ä¸åŒ¹é…): ç¼“å­˜={len(cached_embeddings) if cached_embeddings is not None else 0}, å½“å‰={len(contents)}")
            except Exception as e:
                print(f"   âš ï¸ è¯»å–ç¼“å­˜å‡ºé”™: {e}")

        # è®¡ç®—å‘é‡
        print("   ğŸ”¢ æ­£åœ¨è®¡ç®—æ–‡æœ¬å‘é‡...")
        embeddings = self._encode_with_progress(contents)
        
        # ä¿å­˜ç¼“å­˜
        if use_cache:
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump({
                        'model': self.model_name,
                        'embeddings': embeddings
                    }, f)
                print(f"   ğŸ’¾ å‘é‡å·²ç¼“å­˜åˆ° {cache_path.name}")
            except Exception as e:
                print(f"   âš ï¸ å†™å…¥ç¼“å­˜å‡ºé”™: {e}")
        
        return embeddings
    
    def _encode_with_progress(self, contents: List[str]) -> np.ndarray:
        """å¸¦è¿›åº¦æ¡çš„å‘é‡ç¼–ç ã€‚"""
        batch_size = 32
        all_embeddings = []
        
        for i in tqdm(range(0, len(contents), batch_size), desc="   å‘é‡åŒ–è¿›åº¦"):
            batch = contents[i:i + batch_size]
            batch_embeddings = self.model.encode(batch, show_progress_bar=False)
            all_embeddings.append(batch_embeddings)
        
        return np.vstack(all_embeddings)
    
    def _cluster(self, embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        K-Means èšç±»ã€‚
        
        è¿”å›:
            (cluster_labels, cluster_centers)
        """
        print(f"   ğŸ¯ æ­£åœ¨è¿›è¡Œ K-Means èšç±» (k={self.n_clusters})...")
        
        kmeans = KMeans(
            n_clusters=self.n_clusters,
            random_state=42,
            n_init=10
        )
        labels = kmeans.fit_predict(embeddings)
        
        print("   âœ“ èšç±»å®Œæˆ")
        return labels, kmeans.cluster_centers_
    
    def _reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:
        """
        t-SNE é™ç»´è‡³ 2Dã€‚
        
        è¿”å›:
            2D åæ ‡çŸ©é˜µ (n_samples, 2)
        """
        print("   ğŸ“ æ­£åœ¨è¿›è¡Œ t-SNE é™ç»´...")
        
        # æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´ perplexity
        n_samples = embeddings.shape[0]
        perplexity = min(30, max(5, n_samples // 5))
        
        tsne = TSNE(
            n_components=2,
            perplexity=perplexity,
            random_state=42,
            max_iter=1000  # æ–°ç‰ˆ scikit-learn ä½¿ç”¨ max_iter
        )
        coords = tsne.fit_transform(embeddings)
        
        print("   âœ“ é™ç»´å®Œæˆ")
        return coords
    
    def _build_result(
        self,
        df: pd.DataFrame,
        embeddings: np.ndarray,
        labels: np.ndarray,
        centers: np.ndarray,
        coords: np.ndarray,
        vis_indices: np.ndarray
    ) -> Dict:
        """æ„å»ºåˆ†æç»“æœã€‚"""
        # æ•£ç‚¹å›¾æ•°æ® (åŸºäºé‡‡æ ·åçš„ indices)
        scatter_data = []
        for i, original_idx in enumerate(vis_indices):
            scatter_data.append({
                'x': float(coords[i, 0]),
                'y': float(coords[i, 1]),
                'cluster_id': int(labels[original_idx]),
                'content': df.iloc[original_idx]['content'][:100],  # æˆªæ–­è¿‡é•¿å†…å®¹
                'user': df.iloc[original_idx]['user']
            })
        
        # æ¯ä¸ªèšç±»çš„ä»£è¡¨æ€§æ¶ˆæ¯ï¼ˆè·ç¦»ä¸­å¿ƒæœ€è¿‘çš„æ¶ˆæ¯ï¼‰- ä½¿ç”¨å…¨é‡æ•°æ®è®¡ç®—
        cluster_representatives = self._find_representatives(
            df, embeddings, labels, centers
        )
        
        # èšç±»ç»Ÿè®¡ - ä½¿ç”¨å…¨é‡æ•°æ®
        cluster_stats = []
        for c in range(self.n_clusters):
            cluster_mask = labels == c
            cluster_stats.append({
                'cluster_id': c,
                'count': int(cluster_mask.sum()),
                'name': f'è¯é¢˜ {c + 1}'  # é»˜è®¤åç§°ï¼Œåç»­ç”± AI å‘½å
            })
        
        return {
            'scatter_data': scatter_data,
            'cluster_representatives': cluster_representatives,
            'cluster_stats': cluster_stats,
            'n_clusters': self.n_clusters,
            'total_analyzed': len(df)
        }
    
    def _find_representatives(
        self,
        df: pd.DataFrame,
        embeddings: np.ndarray,
        labels: np.ndarray,
        centers: np.ndarray,
        n_reps: int = 10
    ) -> Dict[int, List[Dict]]:
        """æ‰¾å‡ºæ¯ä¸ªèšç±»çš„ä»£è¡¨æ€§æ¶ˆæ¯ï¼ˆè·ç¦»ä¸­å¿ƒæœ€è¿‘ï¼‰ã€‚"""
        representatives = {}
        
        for c in range(self.n_clusters):
            cluster_mask = labels == c
            cluster_indices = np.where(cluster_mask)[0]
            
            if len(cluster_indices) == 0:
                representatives[c] = []
                continue
            
            # è®¡ç®—åˆ°ä¸­å¿ƒçš„è·ç¦»
            cluster_embeddings = embeddings[cluster_mask]
            distances = np.linalg.norm(cluster_embeddings - centers[c], axis=1)
            
            # é€‰æ‹©æœ€è¿‘çš„ n_reps æ¡
            n_select = min(n_reps, len(cluster_indices))
            nearest_indices = np.argsort(distances)[:n_select]
            
            reps = []
            for idx in nearest_indices:
                orig_idx = cluster_indices[idx]
                reps.append({
                    'content': df.iloc[orig_idx]['content'],
                    'user': df.iloc[orig_idx]['user'],
                    'distance': float(distances[idx])
                })
            
            representatives[c] = reps
        
        return representatives
    
    def _compute_cache_key(self, contents: List[str]) -> str:
        """è®¡ç®—å†…å®¹åˆ—è¡¨çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜ã€‚"""
        # ä½¿ç”¨ é•¿åº¦ + å‰100æ¡ + å100æ¡ ç»„åˆè®¡ç®—å“ˆå¸Œï¼Œå…¼é¡¾é€Ÿåº¦å’Œå‡†ç¡®æ€§
        combined = str(len(contents)) + ''.join(contents[:100]) + ''.join(contents[-100:])
        return hashlib.md5(combined.encode()).hexdigest()[:12]
    
    def _empty_result(self) -> Dict:
        """è¿”å›ç©ºç»“æœã€‚"""
        return {
            'scatter_data': [],
            'cluster_representatives': {},
            'cluster_stats': [],
            'n_clusters': 0,
            'total_analyzed': 0
        }
    
    def analyze_users_for_mbti(
        self,
        df: pd.DataFrame,
        top_users: List[str],
        n_components: int = 24,
        use_cache: bool = True
    ) -> Dict[str, Dict]:
        """
        ä¸ºç”¨æˆ·ç”»åƒç”ŸæˆåŸºäºè¯­ä¹‰ embedding çš„ MBTI åˆ†æå‘é‡ã€‚
        
        ä½¿ç”¨ PCAï¼ˆé t-SNE/UMAPï¼‰è¿›è¡Œé™ç»´ï¼Œä¿è¯ç»“æœç¨³å®šå¯å¤ç°ã€‚
        
        å‚æ•°:
            df: æ¶ˆæ¯æ•°æ® DataFrameï¼ˆéœ€åŒ…å« user, content åˆ—ï¼‰
            top_users: éœ€è¦åˆ†æçš„ç”¨æˆ·åˆ—è¡¨
            n_components: PCA é™ç»´ç›®æ ‡ç»´åº¦ï¼ˆé»˜è®¤ 24ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨å‘é‡ç¼“å­˜
            
        è¿”å›:
            {
                'user_name': {
                    'mean_vector': List[float],      # ç”¨æˆ·å‘è¨€å‡å€¼å‘é‡ï¼ˆé™ç»´åï¼‰
                    'std_value': float,              # å‘è¨€é£æ ¼ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®å‡å€¼ï¼‰
                    'message_count': int,            # å‘è¨€æ•°é‡
                    'topic_distribution': Dict,      # ç”¨æˆ·åœ¨å„è¯é¢˜ç°‡ä¸­çš„åˆ†å¸ƒæ¯”ä¾‹
                    'style_features': Dict,          # è¯­ä¹‰é£æ ¼ç‰¹å¾æè¿°
                },
                ...
            }
        """
        from sklearn.decomposition import PCA
        
        print(f"   ğŸ§¬ æ­£åœ¨è®¡ç®—ç”¨æˆ·è¯­ä¹‰ç‰¹å¾ (åˆ†æ {len(top_users)} ä½ç”¨æˆ·)...")
        
        # è¿‡æ»¤æœ‰æ•ˆæ¶ˆæ¯
        valid_df = self._filter_valid_messages(df)
        if len(valid_df) < 100:
            print("   âš ï¸ æœ‰æ•ˆæ¶ˆæ¯ä¸è¶³ï¼Œè·³è¿‡ç”¨æˆ·è¯­ä¹‰åˆ†æ")
            return {}
        
        # è·å–æ‰€æœ‰æ¶ˆæ¯çš„ embedding
        contents = valid_df['content'].tolist()
        embeddings = self._get_embeddings(contents, use_cache)
        
        # è¿›è¡Œèšç±»ä»¥è·å–è¯é¢˜åˆ†å¸ƒ
        cluster_labels, cluster_centers = self._cluster(embeddings)
        
        # ä¸ºæ¯ä¸ªç”¨æˆ·è®¡ç®—å‘é‡ç‰¹å¾
        user_vectors = {}
        
        for user in tqdm(top_users, desc="   ç”¨æˆ·å‘é‡", ncols=60):
            # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰æ¶ˆæ¯ç´¢å¼•
            user_mask = valid_df['user'] == user
            user_indices = np.where(user_mask)[0]
            
            if len(user_indices) < 5:
                # æ¶ˆæ¯å¤ªå°‘ï¼Œè·³è¿‡
                continue
            
            # è·å–ç”¨æˆ·çš„æ‰€æœ‰ embedding
            user_embeddings = embeddings[user_indices]
            
            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            mean_embedding = np.mean(user_embeddings, axis=0)
            std_embedding = np.std(user_embeddings, axis=0)
            std_value = float(np.mean(std_embedding))  # é£æ ¼ç¨³å®šæ€§æŒ‡æ ‡
            
            # è®¡ç®—è¯é¢˜åˆ†å¸ƒ
            user_labels = cluster_labels[user_indices]
            topic_counts = {}
            for c in range(self.n_clusters):
                count = int(np.sum(user_labels == c))
                if count > 0:
                    topic_counts[c] = count
            
            # è½¬æ¢ä¸ºæ¯”ä¾‹
            total = len(user_indices)
            topic_distribution = {
                k: round(v / total, 3) 
                for k, v in sorted(topic_counts.items(), key=lambda x: -x[1])
            }
            
            # æ‹¼æ¥å‡å€¼ä¸æ ‡å‡†å·®å½¢æˆç‰¹å¾å‘é‡
            combined_vector = np.concatenate([mean_embedding, std_embedding])
            
            user_vectors[user] = {
                'raw_vector': combined_vector,  # ä¸´æ—¶å­˜å‚¨ï¼Œåç»­ PCA å¤„ç†
                'std_value': std_value,
                'message_count': len(user_indices),
                'topic_distribution': topic_distribution,
            }
        
        if not user_vectors:
            print("   âš ï¸ æ²¡æœ‰è¶³å¤Ÿæ•°æ®ç”Ÿæˆç”¨æˆ·å‘é‡")
            return {}
        
        # ä½¿ç”¨ PCA å¯¹æ‰€æœ‰ç”¨æˆ·å‘é‡è¿›è¡Œé™ç»´
        print(f"   ğŸ“ æ­£åœ¨è¿›è¡Œ PCA é™ç»´ (ç›®æ ‡ç»´åº¦: {n_components})...")
        
        user_names = list(user_vectors.keys())
        raw_vectors = np.array([user_vectors[u]['raw_vector'] for u in user_names])
        
        # ç¡®ä¿ n_components ä¸è¶…è¿‡ç‰¹å¾æ•°å’Œæ ·æœ¬æ•°
        actual_components = min(n_components, raw_vectors.shape[0], raw_vectors.shape[1])
        
        pca = PCA(n_components=actual_components, random_state=42)
        pca_vectors = pca.fit_transform(raw_vectors)
        
        print(f"   âœ“ PCA å®Œæˆï¼Œè§£é‡Šæ–¹å·®æ¯”ä¾‹: {sum(pca.explained_variance_ratio_):.2%}")
        
        # è®¡ç®—å…¨ä½“ç”¨æˆ·çš„å‡å€¼å‘é‡ï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        all_mean = np.mean(pca_vectors, axis=0)
        
        # æ›´æ–°ç”¨æˆ·å‘é‡ï¼Œæ·»åŠ é™ç»´åçš„ç»“æœ
        for i, user in enumerate(user_names):
            pca_vec = pca_vectors[i]
            
            # è®¡ç®—ä¸ç¾¤ä½“å¹³å‡çš„åç¦»ç¨‹åº¦
            deviation = float(np.linalg.norm(pca_vec - all_mean))
            
            # ç”Ÿæˆé£æ ¼ç‰¹å¾æè¿°
            std_val = user_vectors[user]['std_value']
            style_features = {
                'stability': 'ç¨³å®š' if std_val < 0.3 else ('å¤šå˜' if std_val > 0.5 else 'é€‚ä¸­'),
                'deviation': 'ç‹¬ç‰¹' if deviation > np.median([np.linalg.norm(pca_vectors[j] - all_mean) for j in range(len(user_names))]) else 'åˆç¾¤',
                'main_topics': list(user_vectors[user]['topic_distribution'].keys())[:3],
            }
            
            # æ›´æ–°å­—å…¸ï¼Œç§»é™¤ä¸´æ—¶ raw_vector
            user_vectors[user] = {
                'mean_vector': pca_vec.tolist(),
                'std_value': user_vectors[user]['std_value'],
                'message_count': user_vectors[user]['message_count'],
                'topic_distribution': user_vectors[user]['topic_distribution'],
                'style_features': style_features,
                'deviation_score': round(deviation, 3),
            }
        
        print(f"   âœ“ å·²ç”Ÿæˆ {len(user_vectors)} ä½ç”¨æˆ·çš„è¯­ä¹‰ç‰¹å¾å‘é‡")
        return user_vectors


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import sys
    from data_loader import load_chat_data
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python vector_engine.py <json_file_path>")
        sys.exit(1)
    
    file_path = sys.argv[1]
    
    try:
        df, session = load_chat_data(file_path)
        print(f"\n=== å‘é‡åˆ†ææµ‹è¯• ===")
        print(f"æ¶ˆæ¯æ€»æ•°: {len(df)}")
        
        analyzer = SemanticAnalyzer(n_clusters=5)
        result = analyzer.analyze(df)
        
        print(f"\n=== åˆ†æç»“æœ ===")
        print(f"åˆ†ææ¶ˆæ¯æ•°: {result['total_analyzed']}")
        print(f"èšç±»æ•°é‡: {result['n_clusters']}")
        
        for stat in result['cluster_stats']:
            print(f"  - {stat['name']}: {stat['count']} æ¡æ¶ˆæ¯")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
