#!/usr/bin/env python3
"""
main.py - å¾®ä¿¡ç¾¤èŠåˆ†æå·¥å…·ä¸»ç¨‹åº

å°†å„æ¨¡å—ä¸²è”ï¼Œå®ç°å®Œæ•´çš„åˆ†ææµç¨‹ï¼š
1. åŠ è½½æ•°æ®
2. ç»Ÿè®¡åˆ†æ
3. å‘é‡è¯­ä¹‰åˆ†æ
4. AI æ·±åº¦åˆ†æï¼ˆç»Ÿä¸€è°ƒåº¦ï¼‰
5. ç”ŸæˆæŠ¥å‘Š
"""

import argparse
import sys
from pathlib import Path
import os

# === å…¨å±€ç¼“å­˜é…ç½® ===
# å¿…é¡»åœ¨å¯¼å…¥ transformers/sentence_transformers ä¹‹å‰è®¾ç½®
PROJECT_ROOT = Path(__file__).resolve().parent
CACHE_DIR = PROJECT_ROOT / ".cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

os.environ["HF_HOME"] = str(CACHE_DIR / "huggingface")
os.environ["SENTENCE_TRANSFORMERS_HOME"] = str(CACHE_DIR / "models")
# ====================
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from src.data_loader import load_chat_data
from src.stats_engine import calculate_stats, format_stats_for_display, calculate_memories_stats
from src.ai import AIAnalyzer
from src.vector_engine import SemanticAnalyzer
from src.poster_builder import generate_poster_report
from src.analyzers import get_monthly_analysis, get_yearly_highlights
from src.analyzers.weekly_analyzer import get_weekly_samples_for_ai


def run_ai_analysis(df, stats, vector_data, args):
    """
    ç»Ÿä¸€çš„ AI åˆ†æè°ƒåº¦å™¨ã€‚
    
    è¿”å›:
        {
            'topic_memories': [...],
            'user_profiles_mbti': [...],
            'weekly_ai_summary': '...',
            'refined_keywords': [...],
        }
    """
    result = {
        'topic_memories': [],
        'user_profiles_mbti': [],
        'weekly_ai_summary': '',
        'refined_keywords': None,
    }
    
    if args.no_ai:
        return result
    
    print("\nğŸ§  æ­£åœ¨è¿›è¡Œ AI æ·±åº¦åˆ†æ...")
    analyzer = AIAnalyzer()
    
    if args.mock:
        analyzer.mock_mode = True
    
    if analyzer.mock_mode:
        print("   âš ï¸ ä½¿ç”¨ Mock æ¨¡å¼ï¼ˆæœªé…ç½® API Keyï¼‰")
    else:
        print(f"   âœ“ ä½¿ç”¨æ¨¡å‹: {analyzer.model}")
    
    # 1. å‘¨åº¦åˆ†æ
    print("   ğŸ“Š [1/4] æ­£åœ¨è¿›è¡Œå‘¨åº¦å…¨é‡æ‰«æ...")
    weekly_samples = get_weekly_samples_for_ai(df, max_per_week=1000)
    weekly_ai_summary, weekly_summaries_dict = analyzer.analyze_weekly_batches(weekly_samples)
    result['weekly_ai_summary'] = weekly_ai_summary
    print("   âœ“ å‘¨åº¦æ·±åº¦æ€»ç»“å·²ç”Ÿæˆ")
    
    # 2. æœˆåº¦è¯é¢˜å›å¿†
    monthly_data = get_monthly_analysis(df)
    if monthly_data:
        print("   ğŸ“… [2/4] æ­£åœ¨ç”Ÿæˆæœˆåº¦è¯é¢˜å›å¿†...")
        if weekly_summaries_dict:
            result['topic_memories'] = analyzer.generate_monthly_summary_from_weekly(
                monthly_data, weekly_summaries_dict
            )
        else:
            result['topic_memories'] = analyzer.generate_topic_memories(monthly_data)
        print(f"   âœ“ å·²ç”Ÿæˆ {len(result['topic_memories'])} ä¸ªæœˆçš„è¯é¢˜å›å¿†")
    
    # 3. ç”¨æˆ·ç”»åƒåŠ MBTI
    print("   ğŸ‘¥ [3/4] æ­£åœ¨ç”Ÿæˆç”¨æˆ·ç”»åƒ...")
    user_counts = df['user'].value_counts()
    all_users = user_counts.index.tolist()
    
    if all_users:
        # è®¡ç®—ç”¨æˆ·è¯­ä¹‰ç‰¹å¾å‘é‡
        user_mbti_vectors = None
        if vector_data and vector_data.get('total_analyzed', 0) > 0:
            try:
                print("   ğŸ§¬ æ­£åœ¨è®¡ç®—ç”¨æˆ·è¯­ä¹‰ç‰¹å¾å‘é‡...")
                semantic_analyzer = SemanticAnalyzer(n_clusters=vector_data.get('n_clusters', 6))
                user_mbti_vectors = semantic_analyzer.analyze_users_for_mbti(df, all_users[:30])
            except Exception as e:
                print(f"   âš ï¸ ç”¨æˆ·è¯­ä¹‰ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        
        result['user_profiles_mbti'] = analyzer.generate_user_profiles_with_mbti(
            df, all_users, user_vectors=user_mbti_vectors
        )
        print(f"   âœ“ å·²ç”Ÿæˆ {len(result['user_profiles_mbti'])} ä½ç”¨æˆ·çš„ MBTI ç”»åƒ")
    
    # 4. å¹´åº¦å…³é”®è¯ä¼˜åŒ–
    yearly_data = get_yearly_highlights(df)
    raw_keywords = yearly_data.get('keywords', [])
    if raw_keywords:
        print("   ğŸ·ï¸ [4/4] æ­£åœ¨ç­›é€‰å¹´åº¦å…³é”®è¯...")
        result['refined_keywords'] = analyzer.refine_keywords(raw_keywords)
        print(f"   âœ“ å·²ç­›é€‰ {len(result['refined_keywords'])} ä¸ªå¹´åº¦å…³é”®è¯")
    
    print("   âœ“ AI æ·±åº¦åˆ†æå®Œæˆ")
    return result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¾®ä¿¡ç¾¤èŠåˆ†æå·¥å…· - ç”Ÿæˆå¹´åº¦åˆ†ææŠ¥å‘Š',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python main.py data/chat_export.json
  python main.py data/chat.json -o reports/
  python main.py data/chat.json --no-ai
        '''
    )
    
    parser.add_argument('input', help='å¾®ä¿¡ç¾¤èŠå¯¼å‡ºçš„ JSON æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', default='output', help='è¾“å‡ºç›®å½• (é»˜è®¤: output)')
    parser.add_argument('--no-ai', action='store_true', help='è·³è¿‡ AI åˆ†æï¼Œä»…ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š')
    parser.add_argument('--no-vector', action='store_true', help='è·³è¿‡å‘é‡è¯­ä¹‰åˆ†æï¼ˆåŠ é€Ÿå¤„ç†ï¼‰')
    parser.add_argument('--no-gpu', action='store_true', help='ç¦ç”¨ GPU åŠ é€Ÿï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU')
    parser.add_argument('--clusters', type=int, default=6, help='èšç±»æ•°é‡ (é»˜è®¤: 6)')
    parser.add_argument('--mock', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨ AI Mock æ¨¡å¼')
    parser.add_argument('-v', '--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--music', type=str, default=None, help='æŠ¥å‘Šçš„èƒŒæ™¯éŸ³ä¹ URL')
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    
    if not input_path.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {input_path}")
        sys.exit(1)
    
    print(f"\n{'='*50}")
    print("ğŸ” å¾®ä¿¡ç¾¤èŠåˆ†æå·¥å…·")
    print(f"{'='*50}\n")
    
    try:
        # Step 1: åŠ è½½æ•°æ®
        print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®...")
        df, session_info = load_chat_data(str(input_path))
        group_name = session_info.get('displayName', 'æœªçŸ¥ç¾¤èŠ')
        print(f"   âœ“ ç¾¤åç§°: {group_name}")
        print(f"   âœ“ æœ‰æ•ˆæ¶ˆæ¯: {len(df)} æ¡")
        print(f"   âœ“ å‚ä¸ç”¨æˆ·: {df['user'].nunique()} äºº")
        
        # Step 2: ç»Ÿè®¡åˆ†æ
        print("\nğŸ“Š æ­£åœ¨è¿›è¡Œç»Ÿè®¡åˆ†æ...")
        stats = calculate_stats(df)
        formatted_stats = format_stats_for_display(stats)
        print(f"   âœ“ æ—¶é—´èŒƒå›´: {formatted_stats['overview']['time_range']}")
        print(f"   âœ“ è¯ç—¨æ‹…å½“: {formatted_stats['overview']['top_user']}")
        print(f"   âœ“ æœ€æ´»è·ƒæ—¶æ®µ: {formatted_stats['overview']['most_active_hour']}")
        
        if args.verbose:
            print("\n   ğŸ“ˆ è¯ç—¨æ’è¡Œæ¦œ (Top 5):")
            for i, user in enumerate(stats['top_users'][:5], 1):
                print(f"      {i}. {user['user']}: {user['count']} æ¡")
        
        # Step 3: å‘é‡è¯­ä¹‰åˆ†æ
        vector_data = None
        if args.no_vector:
            print("\nğŸ§  è·³è¿‡å‘é‡è¯­ä¹‰åˆ†æ (--no-vector)")
        else:
            print("\nğŸ§  æ­£åœ¨è¿›è¡Œæ·±åº¦è¯­ä¹‰åˆ†æ...")
            print("   âš ï¸ é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
            
            try:
                semantic_analyzer = SemanticAnalyzer(
                    n_clusters=args.clusters,
                    use_gpu=not args.no_gpu
                )
                vector_data = semantic_analyzer.analyze(df)
                
                if vector_data and vector_data.get('total_analyzed', 0) > 0:
                    print(f"   âœ“ è¯­ä¹‰åˆ†æå®Œæˆï¼Œå…±åˆ†æ {vector_data['total_analyzed']} æ¡æ¶ˆæ¯")
                    print(f"   âœ“ è¯†åˆ«å‡º {vector_data['n_clusters']} ä¸ªè¯é¢˜èšç±»")
                    
                    # ä½¿ç”¨ AI ä¸ºèšç±»å‘½å
                    if not args.no_ai:
                        print("   ğŸ² æ­£åœ¨ä¸ºè¯é¢˜ç”Ÿæˆåç§°...")
                        ai_analyzer_for_naming = AIAnalyzer()
                        if args.mock:
                            ai_analyzer_for_naming.mock_mode = True
                        cluster_names = ai_analyzer_for_naming.summarize_clusters(
                            vector_data['cluster_representatives']
                        )
                        for stat in vector_data['cluster_stats']:
                            cluster_id = stat['cluster_id']
                            if cluster_id in cluster_names:
                                stat['name'] = cluster_names[cluster_id]
                        print("   âœ“ è¯é¢˜å‘½åå®Œæˆ")
                else:
                    print("   âš ï¸ æœ‰æ•ˆæ¶ˆæ¯ä¸è¶³ï¼Œè·³è¿‡è¯­ä¹‰åˆ†æ")
                    vector_data = None
            except Exception as e:
                print(f"   âš ï¸ å‘é‡åˆ†æå¤±è´¥: {e}")
                if args.verbose:
                    import traceback
                    traceback.print_exc()
                vector_data = None
        
        # Step 4: AI æ·±åº¦åˆ†æï¼ˆç»Ÿä¸€è°ƒåº¦ï¼‰
        ai_data = run_ai_analysis(df, stats, vector_data, args)
        
        # Step 5: æ€€æ—§æ•°æ®æŒ–æ˜
        print("\nâ³ æ­£åœ¨æŒ–æ˜æ€€æ—§æ•°æ®...")
        memories_data = None
        try:
            memories_stats = calculate_memories_stats(df, stats['top_users'])
            memories_data = {
                'hot_messages': memories_stats['hot_messages'],
                'peak_day': memories_stats['peak_day'],
                'first_messages': memories_stats['first_messages'],
                'golden_quotes': [],
                'peak_day_summary': ''
            }
            
            # AI ç”„é€‰é‡‘å¥å’Œç”Ÿæˆå·…å³°æ—¥æ‘˜è¦
            if not args.no_ai and memories_stats['hot_messages']:
                print("   ğŸ² æ­£åœ¨ç”„é€‰é‡‘å¥...")
                ai_for_memories = AIAnalyzer()
                if args.mock:
                    ai_for_memories.mock_mode = True
                memories_data['golden_quotes'] = ai_for_memories.select_golden_quotes(
                    memories_stats['hot_messages']
                )
                print(f"   âœ“ å·²ç”„é€‰ {len(memories_data['golden_quotes'])} æ¡é‡‘å¥")
                
                if memories_stats['peak_day'].get('date'):
                    print("   ğŸ† æ­£åœ¨ç”Ÿæˆå·…å³°æ—¥æ‘˜è¦...")
                    memories_data['peak_day_summary'] = ai_for_memories.summarize_peak_day(
                        memories_stats['peak_day']
                    )
                    print("   âœ“ å·…å³°æ—¥æ‘˜è¦å®Œæˆ")
            
            print(f"   âœ“ æ€€æ—§æ•°æ®æŒ–æ˜å®Œæˆ")
            
        except Exception as e:
            print(f"   âš ï¸ æ€€æ—§æ•°æ®æŒ–æ˜å¤±è´¥: {e}")
            if args.verbose:
                import traceback
                traceback.print_exc()
        
        # Step 6: ç”Ÿæˆæµ·æŠ¥å¼æŠ¥å‘Š
        print("\nğŸ¬ æ­£åœ¨ç”Ÿæˆæµ·æŠ¥å¼æŠ¥å‘Š...")
        output_path = generate_poster_report(
            session_info=session_info,
            df=df,
            ai_data=ai_data,
            memories_data=memories_data,
            output_dir=args.output,
            music_url=args.music,
            vector_data=vector_data
        )
        print(f"   âœ“ æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        
        print(f"\n{'='*50}")
        print("âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
        print(f"{'='*50}")
        print(f"\nğŸ“± æŠ¥å‘Šè·¯å¾„: {output_path}")
        print("\nğŸ’¡ æç¤º: ç”¨æµè§ˆå™¨æ‰“å¼€ HTML æ–‡ä»¶å³å¯æŸ¥çœ‹æŠ¥å‘Š")
        print("   å»ºè®®åœ¨æ‰‹æœºä¸Šç«–å±æŸ¥çœ‹ï¼Œæ•ˆæœæ›´ä½³ï¼")
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
